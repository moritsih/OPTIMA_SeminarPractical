{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "import torchmetrics as tm\n",
    "from torchmetrics.segmentation import MeanIoU\n",
    "\n",
    "from config import Config\n",
    "from dataset import *\n",
    "from utils import DiceCELossSplitter, plot_img_label_pred\n",
    "from transforms import ImageVisualizer\n",
    "import wandb\n",
    "\n",
    "import monai\n",
    "from monai.transforms import *\n",
    "\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed_everything(99, workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globally set source domains for SVDNA and for dataset preppning\n",
    "cfg = Config(source_domains = [\"Spectralis\"])\n",
    "cfg.batch_size = 4\n",
    "cfg.epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change experimental setting, check the following:\n",
    "\n",
    "- source domains in the OCTDatasetPrep class\n",
    "- Turn on SVDNA in the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset length:  4956\n",
      "Training set: 584\n",
      "Validation set: 249\n",
      "Test set: 4123\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = OCTDatasetPrep(cfg.train_dir,\n",
    "                                                 source_domains = cfg.source_domains,\n",
    "                                                ).get_datasets(dataset_split=[0.7, 0,3], use_official_testset=True)\n",
    "\n",
    "train_dataset = MakeDataset(train_data, cfg.train_transforms)\n",
    "val_dataset = MakeDataset(val_data, cfg.val_transforms)\n",
    "test_dataset = MakeDataset(test_data, cfg.test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=7, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=7, persistent_workers=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=7, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.plugins.environments import SLURMEnvironment\n",
    "SLURMEnvironment.detect = lambda: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moritz/miniconda3/envs/optima/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "class LitUNetPlusPlus(L.LightningModule):\n",
    "    def __init__(self, cfg, model):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        #self.save_hyperparameters(ignore=['model'])\n",
    "        self.save_hyperparameters()\n",
    "        self.model = model\n",
    "        self.loss_func = DiceCELossSplitter(include_background=False, sigmoid=True, lambda_ce=0.5)\n",
    "\n",
    "        # several metrics\n",
    "\n",
    "        avg = 'micro'\n",
    "        num_classes = 2\n",
    "\n",
    "        # validation\n",
    "        self.val_accuracy = tm.classification.Accuracy(task=\"binary\", num_classes=num_classes)\n",
    "        self.val_f1 = tm.classification.F1Score(task=\"binary\", num_classes=num_classes)\n",
    "        self.val_precision = tm.classification.Precision(task=\"binary\", average=avg, num_classes=num_classes)\n",
    "        self.val_recall = tm.classification.Recall(task=\"binary\", average=avg, num_classes=num_classes)\n",
    "        self.val_specificity = tm.classification.Specificity(task=\"binary\", average=avg, num_classes=num_classes)\n",
    "        self.val_iou = MeanIoU(num_classes=num_classes, per_class=True, include_background=False)\n",
    "\n",
    "        # test\n",
    "        self.test_accuracy = tm.classification.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_f1 = tm.classification.F1Score(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_precision = tm.classification.Precision(task=\"multiclass\", average=avg, num_classes=num_classes)\n",
    "        self.test_recall = tm.classification.Recall(task=\"multiclass\", average=avg, num_classes=num_classes)\n",
    "        self.test_specificity = tm.classification.Specificity(task=\"multiclass\", average=avg, num_classes=num_classes)\n",
    "        self.test_iou = MeanIoU(num_classes=num_classes, per_class=True, include_background=False)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs = batch['img'].to(self.cfg.device)\n",
    "        #labels = sample['label'].to(device)\n",
    "        masks = batch['masks'].to(self.cfg.device)\n",
    "\n",
    "        outputs = self.model(inputs)\n",
    "        dice_loss, ce_loss, total_loss = self.loss_func(outputs, masks)\n",
    "\n",
    "        #print(f\"Total loss: {total_loss.item()} | Dice loss: {dice_loss.item()} | CE loss: {ce_loss.item()}\")\n",
    "\n",
    "        self.log('train_loss_dice', dice_loss.item())\n",
    "        self.log('train_loss_ce', ce_loss.item())\n",
    "        self.log('train_loss_total', total_loss.item())\n",
    "        torch.cuda.empty_cache()\n",
    "        return total_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs = batch['img'].to(self.cfg.device)\n",
    "        #labels = sample['label'].to(device)\n",
    "        masks = batch['masks'].to(self.cfg.device)\n",
    "\n",
    "        outputs = self.model(inputs)\n",
    "        # try splitting losses\n",
    "        dice, ce, total_loss = self.loss_func(outputs, masks)\n",
    "\n",
    "        output_to_save = torch.sigmoid(outputs[:5])\n",
    "\n",
    "        # thresholding\n",
    "        output_to_save[output_to_save > 0.5] = 1\n",
    "        output_to_save[output_to_save <= 0.5] = 0\n",
    "\n",
    "        for i in range(output_to_save.shape[0]):\n",
    "            img_path = os.path.join(cfg.validation_img_path, f\"b{batch_idx}img{i+1}.png\")\n",
    "            cv2.imwrite(img_path, output_to_save[i, :, :, :].permute(1, 2, 0).cpu().numpy() * 255)\n",
    "\n",
    "        self.log('val_loss_total', total_loss)\n",
    "        self.log('val_loss_dice', dice)\n",
    "        self.log('val_loss_ce', ce)\n",
    "\n",
    "        self.log('val_accuracy', self.val_accuracy(outputs, masks.long()))\n",
    "        self.log('val_f1', self.val_f1(outputs, masks.long()))\n",
    "        self.log('val_precision', self.val_precision(outputs, masks.long()))\n",
    "        self.log('val_recall', self.val_recall(outputs, masks.long()))\n",
    "        self.log('val_specificity', self.val_specificity(outputs, masks.long()))\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs = batch['img'].to(self.cfg.device)\n",
    "        #labels = sample['label'].to(device)\n",
    "        masks = batch['masks'].to(self.cfg.device)\n",
    "\n",
    "        outputs = self.model(inputs)\n",
    "        #loss = self.loss_func(outputs, masks)\n",
    "        # try splitting losses\n",
    "        dice, ce, total_loss = self.loss_func(outputs, masks)\n",
    "\n",
    "        torch.sigmoid_(outputs)\n",
    "\n",
    "        # thresholding\n",
    "        outputs[outputs > 0.5] = 1\n",
    "        outputs[outputs <= 0.5] = 0\n",
    "\n",
    "        self.log('test_loss_total', total_loss)\n",
    "        self.log('test_loss_dice', dice)\n",
    "        self.log('test_loss_ce', ce)\n",
    "\n",
    "        self.log('test_accuracy', self.test_accuracy(outputs, masks.long()))\n",
    "        self.log('test_f1', self.test_f1(outputs, masks.long()))\n",
    "        self.log('test_precision', self.test_precision(outputs, masks.long()))\n",
    "        self.log('test_recall', self.test_recall(outputs, masks.long()))\n",
    "        self.log('test_specificity', self.test_specificity(outputs, masks.long()))\n",
    "        #self.log('test_iou', self.test_iou(outputs.long(), masks.long()))\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.cfg.lr, weight_decay=self.cfg.weight_decay)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=self.cfg.factor, patience=self.cfg.patience_lr)\n",
    "        return {\"optimizer\": optimizer, \n",
    "                \"lr_scheduler\": {'scheduler': scheduler, 'monitor': 'val_loss_total'}}\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "model = smp.UnetPlusPlus(**cfg.model_parameters_unetpp)\n",
    "    \n",
    "unetpp = LitUNetPlusPlus(cfg, model)\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"PracticalWorkinAI\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=cfg.model_path, monitor='val_loss_total', mode='min')\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "\n",
    "trainer = L.Trainer(max_epochs=cfg.epochs, \n",
    "                    accumulate_grad_batches=8,\n",
    "                    logger=wandb_logger, \n",
    "                    default_root_dir=Path(cfg.default_root_dir),\n",
    "                    log_every_n_steps=2,\n",
    "                    callbacks=[checkpoint_callback, lr_monitor],\n",
    "                    deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmoritsih\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240526_171936-7ch77r9k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/moritsih/PracticalWorkinAI/runs/7ch77r9k/workspace' target=\"_blank\">avid-wave-151</a></strong> to <a href='https://wandb.ai/moritsih/PracticalWorkinAI' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/moritsih/PracticalWorkinAI' target=\"_blank\">https://wandb.ai/moritsih/PracticalWorkinAI</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/moritsih/PracticalWorkinAI/runs/7ch77r9k/workspace' target=\"_blank\">https://wandb.ai/moritsih/PracticalWorkinAI/runs/7ch77r9k/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moritz/miniconda3/envs/optima/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/moritz/Documents/practicalAI/OPTIMA_Masterarbeit/practical/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name             | Type                  | Params\n",
      "------------------------------------------------------------\n",
      "0  | model            | UnetPlusPlus          | 41.0 M\n",
      "1  | loss_func        | DiceCELossSplitter    | 0     \n",
      "2  | val_accuracy     | BinaryAccuracy        | 0     \n",
      "3  | val_f1           | BinaryF1Score         | 0     \n",
      "4  | val_precision    | BinaryPrecision       | 0     \n",
      "5  | val_recall       | BinaryRecall          | 0     \n",
      "6  | val_specificity  | BinarySpecificity     | 0     \n",
      "7  | val_iou          | MeanIoU               | 0     \n",
      "8  | test_accuracy    | MulticlassAccuracy    | 0     \n",
      "9  | test_f1          | MulticlassF1Score     | 0     \n",
      "10 | test_precision   | MulticlassPrecision   | 0     \n",
      "11 | test_recall      | MulticlassRecall      | 0     \n",
      "12 | test_specificity | MulticlassSpecificity | 0     \n",
      "13 | test_iou         | MeanIoU               | 0     \n",
      "------------------------------------------------------------\n",
      "41.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "41.0 M    Total params\n",
      "163.976   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613554ab182c4987bba8c884a9933b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moritz/miniconda3/envs/optima/lib/python3.10/site-packages/torch/_tensor.py:1443: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  ret = func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3df6adf285f47fbbe33b867c888dc3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n",
      "SVDNA performed.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43munetpp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#trainer.test(unetpp, dataloaders=val_loader)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:183\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# when the strategy handles accumulation, we want to always call the optimizer step\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mhandles_gradient_accumulation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39m_should_accumulate()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# -------------------\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# automatic_optimization=True: perform ddp sync only when performing optimizer_step\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _block_parallel_sync_behavior(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy, block\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 183\u001b[0m         \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step(batch_idx, closure)\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:318\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:391\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 36\u001b[0m, in \u001b[0;36mLitUNetPlusPlus.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#labels = sample['label'].to(device)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m masks \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 36\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m dice_loss, ce_loss, total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_func(outputs, masks)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#print(f\"Total loss: {total_loss.item()} | Dice loss: {dice_loss.item()} | CE loss: {ce_loss.item()}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/segmentation_models_pytorch/base/model.py:30\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input_shape(x)\n\u001b[1;32m     29\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[0;32m---> 30\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_head(decoder_output)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification_head \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/segmentation_models_pytorch/decoders/unetplusplus/decoder.py:135\u001b[0m, in \u001b[0;36mUnetPlusPlusDecoder.forward\u001b[0;34m(self, *features)\u001b[0m\n\u001b[1;32m    133\u001b[0m             cat_features \u001b[38;5;241m=\u001b[39m [dense_x[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdense_l_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, dense_l_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m    134\u001b[0m             cat_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(cat_features \u001b[38;5;241m+\u001b[39m [features[dense_l_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 135\u001b[0m             dense_x[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdepth_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdense_l_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdepth_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdense_l_i\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdense_x\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdepth_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdense_l_i\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m dense_x[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m](dense_x[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dense_x[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/segmentation_models_pytorch/decoders/unetplusplus/decoder.py:39\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[0;34m(self, x, skip)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, skip], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[1;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/segmentation_models_pytorch/base/modules.py:131\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/segmentation_models_pytorch/base/modules.py:63\u001b[0m, in \u001b[0;36mSCSEModule.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcSE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msSE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/monai/data/meta_tensor.py:282\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 282\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "File \u001b[0;32m~/miniconda3/envs/optima/lib/python3.10/site-packages/torch/_tensor.py:1443\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1443\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "trainer.fit(unetpp, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "#trainer.test(unetpp, dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some images and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_some_imgs = False\n",
    "\n",
    "if print_some_imgs:\n",
    "    \n",
    "    transforms_visualize = Compose([cfg.train_transforms, ImageVisualizer(keys=['img', 'label', 'masks'])])\n",
    "    train_data_raw, _, _ = OCTDatasetPrep(cfg.train_dir, source_domains=cfg.source_domains).get_datasets(use_official_testset=False)\n",
    "    train_data = MakeDataset(train_data_raw, transforms_visualize)\n",
    "\n",
    "\n",
    "    for i in range(4):\n",
    " \n",
    "        rand_num = np.random.randint(0, len(train_data))\n",
    "        sample = train_data[rand_num]\n",
    "        print(f\"Sample {i+1}\", \n",
    "              \"\\nImg: \", sample['img'].shape,\n",
    "              \"\\nLabel: \", sample['label'].shape,\n",
    "              \"\\nMasks: \", sample['masks'].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and make predictions on random images from validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['untrained_baseline.ckpt']\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = os.listdir(cfg.model_path)\n",
    "print(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions = False\n",
    "\n",
    "if get_predictions:\n",
    "    model = LitUNetPlusPlus.load_from_checkpoint(checkpoint_path = cfg.model_path / model_checkpoints[0])\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(np.min([5, len(val_dataset)])):\n",
    "        rand_num = np.random.randint(0, len(val_dataset))\n",
    "        sample = val_dataset[rand_num]\n",
    "\n",
    "        img = sample['img'].unsqueeze(0).to(cfg.device)\n",
    "        mask = sample['masks'].unsqueeze(0).to(cfg.device)\n",
    "\n",
    "        pred = model(img)\n",
    "        pred = torch.sigmoid(pred)\n",
    "        \n",
    "        # thresholding\n",
    "        pred[pred > 0.5] = 1\n",
    "        pred[pred <= 0.5] = 0\n",
    "\n",
    "        plot_img_label_pred(img, pred, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fea8962758740628714e8e3eb4d279b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef8d94bd7b04916a77194683669d7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4a026310b6487aaf5b5e93eb627ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9cc6b4b7c24f9f901024d8ff1670e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#model = LitUNetPlusPlus.load_from_checkpoint(checkpoint_path = cfg.model_path / model_checkpoints[0])\n",
    "#table.append(make_results_table(model, val_dataset, cfg.device, cfg, 'ImageNet pretrained'))\n",
    "\n",
    "#model = LitUNetPlusPlus.load_from_checkpoint(checkpoint_path = cfg.model_path / model_checkpoints[0])\n",
    "#table.append(make_results_table(model, val_dataset, cfg.device, cfg, 'Supervised training'))\n",
    "\n",
    "#model = LitUNetPlusPlus.load_from_checkpoint(checkpoint_path = cfg.model_path / model_checkpoints[0])\n",
    "#table.append(make_results_table(model, val_dataset, cfg.device, cfg, 'SVDNA source: Spectralis'))\n",
    "\n",
    "#model = LitUNetPlusPlus.load_from_checkpoint(checkpoint_path = cfg.model_path / model_checkpoints[0])\n",
    "#table.append(make_results_table(model, val_dataset, cfg.device, cfg, 'SVDNA source: Cirrus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|             |           | Model   | IRF      | SRF         | PED      |\n",
      "|-------------|-----------|---------|----------|-------------|----------|\n",
      "| dice_scores | precision | recall  | accuracy | specificity | f1 score |\n",
      "| dice_scores | precision | recall  | accuracy | specificity | f1 score |\n",
      "| dice_scores | precision | recall  | accuracy | specificity | f1 score |\n",
      "| dice_scores | precision | recall  | accuracy | specificity | f1 score |\n"
     ]
    }
   ],
   "source": [
    "# save variable table to text file\n",
    "#print(tabulate(table, headers='firstrow', tablefmt='github'))\n",
    "#with open('results.txt', 'w') as f:\n",
    "#    f.write(tabulate(table, headers='firstrow', tablefmt='latex_raw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make a barplot with three bars, one for each class\n",
    "\n",
    "'''fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "x = np.arange(3)\n",
    "width = 0.15\n",
    "\n",
    "for i, model in enumerate(table[1:]):\n",
    "    ax.bar(x + i*width, model[1:], 0.15, label=model[0])\n",
    "\n",
    "ax.set_xticks(x + width*1.5)\n",
    "#ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "ax.set_xticklabels(['IRF', 'SRF', 'PED'])\n",
    "ax.set_ylabel('Dice score')\n",
    "ax.set_title('Mean dice score for each class')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

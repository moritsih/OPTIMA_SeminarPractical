{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import monai\n",
    "import cv2\n",
    "import torch\n",
    "from monai.transforms import *\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "#import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "'''\n",
    "1. RETOUCH dataset\n",
    "\n",
    "    Preprocessing\n",
    "    - Preprocessed dataset for now\n",
    "\n",
    "    Training/validation/test\n",
    "    - split train folder into 75/15/10\n",
    "    - Use Topcon&Cirrus for validation and testing too? Or use ONLY Topcon&Cirrus for val&test?\n",
    "        > Paper uses only topcon and cirrus for validation and testing\n",
    "\n",
    "    SVDNA\n",
    "    - Create Pytorch Dataset class:\n",
    "        > define __getitem__ method such that I give it the path to the training set and it returns the SVDNA transformed images\n",
    "\n",
    "    - when epoch starts, SVDNA is applied to each image\n",
    "        > for each image, 1/3 chance to choose one of the domains\n",
    "            > of the n_d images, one is chosen randomly for style transfer\n",
    "        > for each image, k is sampled between 30 and 50\n",
    "        > apply SVDNA using sampled image and k\n",
    "    - Implementation:\n",
    "        > dict containing spectralis images\n",
    "        > dict containing cirrus and topcon images\n",
    "\n",
    "        for epoch in total_epochs:\n",
    "            source_imgs = spectralis_img\n",
    "            target_imgs = cirrus_topcon_img\n",
    "\n",
    "            source_svdna = [svdna(imgs[i], target_imgs):labels[i] for imgs, labels in source_imgs.items()]\n",
    "\n",
    "            img_dataloader = Dataloader(source_svdna)\n",
    "\n",
    "    Transformations\n",
    "    - \n",
    "\n",
    "\n",
    "    - Possibilities for datastructure:\n",
    "        - dict(source:{Cirrus:{images, labels}}, target{Spectralis:images, Topcon:images } )\n",
    "        - dict(Cirrus:{images, labels}, Spectralis:images, Topcon:images)\n",
    "\n",
    "    \n",
    "        \n",
    "https://docs.monai.io/en/stable/networks.html#basicunetplusplus\n",
    "\n",
    "\n",
    "First of all, I need to get the data and create a pipeline for using transformations during training.\n",
    "I need to create a pipeline for the training data and another for the validation data.\n",
    "I need to get a UNet++ from monai and train it with the training data.\n",
    "I need to get SVDNA and apply it to the data as the first transformation\n",
    "Finally, during training, first SVDNA is applied, then any other transformation.\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "base_dir = Path(os.getcwd())\n",
    "data_dir = base_dir / 'data' / 'Retouch-preprocessed'\n",
    "\n",
    "def get_data_dict(data_dir = Path(os.getcwd()) / 'data' / 'Retouch-preprocessed', dataset = 'train'):\n",
    "    \n",
    "    dataset_dir = data_dir / dataset\n",
    "    \n",
    "\n",
    "get_data_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1024, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_img = '/Users/moritz/Documents/Master/OPTIMA_Masterarbeit/data/RETOUCH/TrainingSet-Release/Cirrus/03a60d9078d35b1488e6030880a29014/reference.mhd'\n",
    "itk_image = sitk.ReadImage(sample_img)\n",
    "image_array = sitk.GetArrayViewFromImage(itk_image)\n",
    "\n",
    "# print the image's dimensions\n",
    "print(image_array.shape)\n",
    "\n",
    "# plot the image\n",
    "for i in range(128):\n",
    "    if i % 10 == 0:\n",
    "        pass\n",
    "        #plt.imshow(image_array[i], cmap='gray')\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dir = Path('/Users/moritz/Documents/Master/OPTIMA_Masterarbeit/data/RETOUCH/TrainingSet-Release/')\n",
    "train_dir = Path('/Users/moritz/Documents/Master/OPTIMA_Masterarbeit/data/Retouch-Preprocessed/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  Topcon ; Number of folders:  22\n",
      "Device:  Spectralis ; Number of folders:  24\n",
      "Device:  Cirrus ; Number of folders:  24\n",
      "Preprocessed Retouch folders:  70\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "for device in os.listdir(name_dir):\n",
    "    data[device] = os.listdir(name_dir / device)\n",
    "\n",
    "for device, vals in data.items():\n",
    "    print(\"Device: \", device, \"; Number of folders: \", len(vals))\n",
    "\n",
    "print(\"Preprocessed Retouch folders: \", len(os.listdir(train_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_black_images(main_folder, delete_images=False):\n",
    "    \"\"\"\n",
    "    Generate black images for missing files in the label_image folder.\n",
    "\n",
    "    Args:\n",
    "        main_folder (str or Path): Path to the main folder.\n",
    "        delete_images (bool, optional): Flag to delete the generated black images. Defaults to False.\n",
    "    \"\"\"\n",
    "    main_folder = Path(main_folder)\n",
    "\n",
    "    if not delete_images:\n",
    "    # Iterate over the subfolders\n",
    "        for subfolder in sorted(os.listdir(main_folder)):\n",
    "            subfolder_path = main_folder / subfolder\n",
    "\n",
    "            # Check if the subfolder contains the 'image' and 'label_image' folders\n",
    "            if os.path.isdir(subfolder_path) and 'image' in os.listdir(subfolder_path) and 'label_image' in os.listdir(subfolder_path):\n",
    "                image_folder = subfolder_path / 'image'\n",
    "                label_folder = subfolder_path / 'label_image'\n",
    "\n",
    "                # Get the set of filenames in the 'image' folder\n",
    "                image_files = sorted(set(os.listdir(image_folder)))\n",
    "\n",
    "                # Get the set of filenames in the 'label_image' folder\n",
    "                label_files = sorted(set(os.listdir(label_folder)))\n",
    "\n",
    "                # Find the filenames that are in 'label_image' but not in 'image'\n",
    "                missing_files = [i for i in image_files if i not in label_files]\n",
    "\n",
    "                # Create a black image for each missing file\n",
    "                for file in missing_files:\n",
    "                    file_path = label_folder / file\n",
    "\n",
    "                    # find the shape of the input image to create corresponding target\n",
    "                    file_shape = cv2.imread(str(image_folder / file)).shape\n",
    "                    black_image = np.zeros(file_shape)\n",
    "\n",
    "                    # make unique names so files can be deleted again\n",
    "                    cv2.imwrite(f\"{str(file_path)[:-4]}_empty.png\", black_image)\n",
    "\n",
    "    # Delete the generated black images if delete_images flag is True\n",
    "    if delete_images:\n",
    "        for subfolder in sorted(os.listdir(main_folder)):\n",
    "            subfolder_path = main_folder / subfolder\n",
    "            if os.path.isdir(subfolder_path) and 'label_image' in os.listdir(subfolder_path):\n",
    "                label_folder = subfolder_path / 'label_image'\n",
    "                for file in os.listdir(label_folder):\n",
    "                    file_path = label_folder / file\n",
    "                    if \"_empty\" in str(file):\n",
    "                        os.remove(str(file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_black_images(train_dir, delete_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import Compose, Randomizable, RandomizableTransform\n",
    "from monai.transforms import LoadNiftid, AddChanneld, ScaleIntensityd, ToTensord, RandFlipd, RandRotate90d\n",
    "import wandb\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"PracticalWorkinAI\", name=\"svdna_reproduction_retouch_only\")\n",
    "\n",
    "# Define the SVDNA function\n",
    "def svdna(k,target_path,src_path,histo_matching_degree=0.5):\n",
    "\n",
    "    target_img = Image.open(target_path).convert(\"L\")\n",
    "    src_img = Image.open(src_path).convert(\"L\")\n",
    "\n",
    "\n",
    "    resized_target=np.array(target_img.resize((IMAGE_SIZE,IMAGE_SIZE), Image.NEAREST))\n",
    "    resized_src=np.array(src_img.resize((IMAGE_SIZE,IMAGE_SIZE), Image.NEAREST))\n",
    "\n",
    "    u_target,s_target,vh_target=np.linalg.svd(resized_target,full_matrices=False)\n",
    "    u_source,s_source,vh_source=np.linalg.svd(resized_src,full_matrices=False)\n",
    "\n",
    "    thresholded_singular_target=s_target\n",
    "    thresholded_singular_target[0:k]=0\n",
    "\n",
    "    thresholded_singular_source=s_source\n",
    "    thresholded_singular_source[k:]=0\n",
    "\n",
    "    target_style=np.array([np.dot(u_target, np.dot(np.diag(thresholded_singular_target), vh_target))])\n",
    "\n",
    "    content_src=np.array([np.dot(u_source, np.dot(np.diag(thresholded_singular_source), vh_source))])\n",
    "    content_trgt=resized_target-target_style\n",
    "\n",
    "    noise_adapted_im=content_src+target_style\n",
    "\n",
    "    noise_adapted_im_clipped=np.squeeze(noise_adapted_im).clip(0,255).astype(np.uint8)\n",
    "\n",
    "    transformHist = A.Compose([\n",
    "        A.HistogramMatching([target_path], blend_ratio=(histo_matching_degree, histo_matching_degree), read_fn=readIm, p=1)\n",
    "    ])\n",
    "\n",
    "    image = np.array(Image.open(src_path).resize((IMAGE_SIZE,IMAGE_SIZE)))\n",
    "\n",
    "    transformed = transformHist(image=noise_adapted_im_clipped)\n",
    "    svdna_im = transformed[\"image\"]\n",
    "\n",
    "    return resized_src,resized_target,content_src,np.squeeze(target_style), svdna_im,noise_adapted_im_clipped\n",
    "\n",
    "\n",
    "# Define the custom dataset class\n",
    "class OCTDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "\n",
    "        self.transform = transform\n",
    "        self.source_data_paths, self.target_data_paths = self.filter_source_domain(data_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_data_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_img = load_source_image(self.source_data_paths[index])\n",
    "        target_img = load_target_image(self.target_data_paths[index])\n",
    "        \n",
    "        if self.transform:\n",
    "            # Apply transformations dynamically during training\n",
    "            source_img, target_img = self.transform(source_img, target_img)\n",
    "\n",
    "        return source_img, target_img\n",
    "\n",
    "    def filter_source_domain(self, source_domain):\n",
    "        # go into path:\n",
    "        # '/Users/moritz/Documents/Master/OPTIMA_Masterarbeit/data/RETOUCH/TrainingSet-Release/'\n",
    "        # and set the containing folders as keys of a dictionary. The values are the folder names in the respective\n",
    "        # folder.\n",
    "        # The created dictionary serves as a tool for filtering the folders that contain the source images.\n",
    "        # The folder \"self.data_path\" contains folder names that correspond to the folder names in the dictionary's values.\n",
    "        # The goal is to return a variable source_data_paths containing the paths to the source domain images, while \n",
    "        # the variable target_data_paths contains the paths to the target domain images (Cirrus and Topcon).\n",
    "        # source_data_paths should be a dictionary with the structure {Spectralis: [path1, path2, ...]}\n",
    "        # target_data_paths should be a dictionary with the structure {Cirrus: [path1, path2, ...], Topcon: [path1, path2, ...]}\n",
    "        # The dictionary should be created in the __init__ method of the class.\n",
    "        # The method should return the source_data_paths and target_data_paths variables.\n",
    "\n",
    "        named_domain_folder = Path('/Users/moritz/Documents/Master/OPTIMA_Masterarbeit/data/RETOUCH/TrainingSet-Release/')\n",
    "\n",
    "        domains = os.listdir(named_domain_folder)\n",
    "        target_domains = domains.remove(source_domain)\n",
    "\n",
    "        # creates dict e.g. {'cirrus':['path1', 'path2', ...], 'topcon':['path1', 'path2', ...]}\n",
    "        filter_dict = {domain:os.listdir(named_domain_folder / domain) for domain in os.listdir(named_domain_folder)}\n",
    "\n",
    "\n",
    "        source_data_paths = {source_domain: [if img in self.data_path]}\n",
    "        target_data_paths = {}\n",
    "\n",
    "\n",
    "\n",
    "    def load_source_image(self, source_path):\n",
    "        # Load the source image\n",
    "        return source_img\n",
    "\n",
    "    def load_target_image(self, target_path):\n",
    "        # Load the target image\n",
    "        return target_img\n",
    "\n",
    "\n",
    "    def generate_black_images(self, delete_images=False):\n",
    "        \"\"\"\n",
    "        Generate black images for missing files in the label_image folder.\n",
    "\n",
    "        Args:\n",
    "            main_folder (str or Path): Path to the main folder.\n",
    "            delete_images (bool, optional): Flag to delete the generated black images. Defaults to False.\n",
    "        \"\"\"\n",
    "        main_folder = Path(self.source_data_paths)\n",
    "\n",
    "        if not delete_images:\n",
    "        # Iterate over the subfolders\n",
    "            for subfolder in sorted(os.listdir(main_folder)):\n",
    "                subfolder_path = main_folder / subfolder\n",
    "\n",
    "                # Check if the subfolder contains the 'image' and 'label_image' folders\n",
    "                if os.path.isdir(subfolder_path) and 'image' in os.listdir(subfolder_path) and 'label_image' in os.listdir(subfolder_path):\n",
    "                    image_folder = subfolder_path / 'image'\n",
    "                    label_folder = subfolder_path / 'label_image'\n",
    "\n",
    "                    # Get the set of filenames in the 'image' folder\n",
    "                    image_files = sorted(set(os.listdir(image_folder)))\n",
    "\n",
    "                    # Get the set of filenames in the 'label_image' folder\n",
    "                    label_files = sorted(set(os.listdir(label_folder)))\n",
    "\n",
    "                    # Find the filenames that are in 'label_image' but not in 'image'\n",
    "                    missing_files = [i for i in image_files if i not in label_files]\n",
    "\n",
    "                    # Create a black image for each missing file\n",
    "                    for file in missing_files:\n",
    "                        file_path = label_folder / file\n",
    "\n",
    "                        # find the shape of the input image to create corresponding target\n",
    "                        file_shape = cv2.imread(str(image_folder / file)).shape\n",
    "                        black_image = np.zeros(file_shape)\n",
    "\n",
    "                        # make unique names so files can be deleted again\n",
    "                        cv2.imwrite(f\"{str(file_path)[:-4]}_empty.png\", black_image)\n",
    "\n",
    "        # Delete the generated black images if delete_images flag is True\n",
    "        if delete_images:\n",
    "            for subfolder in sorted(os.listdir(main_folder)):\n",
    "                subfolder_path = main_folder / subfolder\n",
    "                if os.path.isdir(subfolder_path) and 'label_image' in os.listdir(subfolder_path):\n",
    "                    label_folder = subfolder_path / 'label_image'\n",
    "                    for file in os.listdir(label_folder):\n",
    "                        file_path = label_folder / file\n",
    "                        if \"_empty\" in str(file):\n",
    "                            os.remove(str(file_path))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Define the randomizable transformation class for dynamic transformations\n",
    "class RandomTransform(RandomizableTransform):\n",
    "    def __call__(self, source_img, target_img):\n",
    "        self.randomize()\n",
    "        # Implement your dynamic random transformations\n",
    "        # Apply random transformations to both source and target images\n",
    "        return transformed_source_img, transformed_target_img\n",
    "\n",
    "# Define the UNet model\n",
    "class SegmentationUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegmentationUNet, self).__init__()\n",
    "        # Define your UNet architecture using MONAI\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement your forward pass logic\n",
    "        return x\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = SegmentationUNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the dataset paths\n",
    "source_data_paths = [...]  # List of paths to source domain data\n",
    "target_data_paths = [...]  # List of paths to target domain data\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "transform = Compose([\n",
    "    RandomTransform(),\n",
    "    svdna,  # Apply SVDNA function\n",
    "    ToTensord()\n",
    "])\n",
    "\n",
    "dataset = OCTDataset(source_data_paths, target_data_paths, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (source_img, target_img) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(source_img)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, target_img)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log loss to wandb\n",
    "        wandb.log({\"Loss\": loss.item(), \"Epoch\": epoch, \"Batch\": batch_idx})\n",
    "\n",
    "        # Print progress\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item()}')\n",
    "\n",
    "# Save the trained model if needed\n",
    "torch.save(model.state_dict(), 'segmentation_unet.pth')\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['80293194ec21abbfd3da95f83cdbd5ab',\n",
       " '93ae688615be155f7207e4c1f45eb725',\n",
       " 'd40ef678ef3dff06eb3aa3a4f4ae99e6',\n",
       " 'a953b47afe290923d91a08610799ea42',\n",
       " '6a96f7c246314e625bacfb78d18ad09e',\n",
       " '70f44050a581f94509fd63bcbd797971',\n",
       " '92a0bdb6013c8d0b4770e4e907d3d8cf',\n",
       " '8e78a8e83df1029abbd2479ff1d5f92f',\n",
       " 'a6be6cdf56b8354d72483606af62b8bb',\n",
       " '6af50661914d9c03417adbe6eb91ebae',\n",
       " '03a60d9078d35b1488e6030880a29014',\n",
       " '644b104a47cf811b3828eb4655284991',\n",
       " '091e68f597bb8e45ce9478363ef686b3',\n",
       " '42533245b1c2cf49d9d5c554dbdee5a0',\n",
       " '1e0e71d2acdc57f10ab6712ab87b2ef7',\n",
       " '5fef2f4c2adcda3b9e07801f713d4ccf',\n",
       " '6107c818602ec2abf340a89b8e225b12',\n",
       " '8248fbc73c9381d0bf3e909c7d732f84',\n",
       " '8482e2f4d2aae33a5eac5178801df9fb',\n",
       " '7291fbbae825c6a9230b8787ee7645d2',\n",
       " 'b35010885c2127df56140ffbfc3db3e3',\n",
       " '4d1f2722e3f9c55b689621d1228526db',\n",
       " '3c68f67cd2e2b41afa54bf6059f509d1',\n",
       " 'e6747bd39c2fc8a907a7193731724eab']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optima",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

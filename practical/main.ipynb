{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import Compose, Randomizable, RandomizableTransform\n",
    "from monai.transforms import LoadNiftid, AddChanneld, ScaleIntensityd, ToTensord, RandFlipd, RandRotate90d\n",
    "import wandb\n",
    "\n",
    "\n",
    "'''\n",
    "1. RETOUCH dataset\n",
    "\n",
    "    Preprocessing\n",
    "    - Preprocessed dataset for now\n",
    "\n",
    "    Training/validation/test\n",
    "    - split train folder into 75/15/10\n",
    "    - Use Topcon&Cirrus for validation and testing too? Or use ONLY Topcon&Cirrus for val&test?\n",
    "        > Paper uses only topcon and cirrus for validation and testing\n",
    "\n",
    "    SVDNA\n",
    "    - Create Pytorch Dataset class:\n",
    "        > define __getitem__ method such that I give it the path to the training set and it returns the SVDNA transformed images\n",
    "\n",
    "    - when epoch starts, SVDNA is applied to each image\n",
    "        > for each image, 1/3 chance to choose one of the domains\n",
    "            > of the n_d images, one is chosen randomly for style transfer\n",
    "        > for each image, k is sampled between 30 and 50\n",
    "        > apply SVDNA using sampled image and k\n",
    "    - Implementation:\n",
    "        > dict containing spectralis images\n",
    "        > dict containing cirrus and topcon images\n",
    "\n",
    "        for epoch in total_epochs:\n",
    "            source_imgs = spectralis_img\n",
    "            target_imgs = cirrus_topcon_img\n",
    "\n",
    "            source_svdna = [svdna(imgs[i], target_imgs):labels[i] for imgs, labels in source_imgs.items()]\n",
    "\n",
    "            img_dataloader = Dataloader(source_svdna)\n",
    "\n",
    "    Transformations\n",
    "    - \n",
    "\n",
    "\n",
    "    - Possibilities for datastructure:\n",
    "        - [{cirrus_img1:cirrus_img1, cirrus_img1_label:cirrus_img1_label}, {cirrus_img2:cirrus_img2, cirrus_img2_label:cirrus_img2_label}, ...]\n",
    "\n",
    "    \n",
    "        \n",
    "https://docs.monai.io/en/stable/networks.html#basicunetplusplus\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "base_dir = Path(os.getcwd())\n",
    "data_dir = base_dir / 'data' / 'Retouch-preprocessed'\n",
    "\n",
    "def get_data_dict(data_dir = Path(os.getcwd()) / 'data' / 'Retouch-preprocessed', dataset = 'train'):\n",
    "    \n",
    "    dataset_dir = data_dir / dataset\n",
    "    \n",
    "\n",
    "#get_data_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1024, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_img = '/Users/moritz/Documents/Master/OPTIMA_Masterarbeit/data/RETOUCH/TrainingSet-Release/Cirrus/03a60d9078d35b1488e6030880a29014/reference.mhd'\n",
    "itk_image = sitk.ReadImage(sample_img)\n",
    "image_array = sitk.GetArrayViewFromImage(itk_image)\n",
    "\n",
    "# print the image's dimensions\n",
    "print(image_array.shape)\n",
    "\n",
    "# plot the image\n",
    "for i in range(128):\n",
    "    if i % 10 == 0:\n",
    "        pass\n",
    "        #plt.imshow(image_array[i], cmap='gray')\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dir = Path('/Users/moritz/Documents/Master/OPTIMA_Masterarbeit/data/RETOUCH/TrainingSet-Release/')\n",
    "train_dir = Path('/Users/moritz/Documents/Master/OPTIMA_Masterarbeit/data/Retouch-Preprocessed/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  Topcon ; Number of folders:  22\n",
      "Device:  Spectralis ; Number of folders:  24\n",
      "Device:  Cirrus ; Number of folders:  24\n",
      "Preprocessed Retouch folders:  70\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "for device in os.listdir(name_dir):\n",
    "    data[device] = os.listdir(name_dir / device)\n",
    "\n",
    "for device, vals in data.items():\n",
    "    print(\"Device: \", device, \"; Number of folders: \", len(vals))\n",
    "\n",
    "print(\"Preprocessed Retouch folders: \", len(os.listdir(train_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_black_images(main_folder, delete_images=False):\n",
    "    \"\"\"\n",
    "    Generate black images for missing files in the label_image folder.\n",
    "\n",
    "    Args:\n",
    "        main_folder (str or Path): Path to the main folder.\n",
    "        delete_images (bool, optional): Flag to delete the generated black images. Defaults to False.\n",
    "    \"\"\"\n",
    "    main_folder = Path(main_folder)\n",
    "\n",
    "    if not delete_images:\n",
    "    # Iterate over the subfolders\n",
    "        for subfolder in sorted(os.listdir(main_folder)):\n",
    "            subfolder_path = main_folder / subfolder\n",
    "\n",
    "            # Check if the subfolder contains the 'image' and 'label_image' folders\n",
    "            if os.path.isdir(subfolder_path) and 'image' in os.listdir(subfolder_path) and 'label_image' in os.listdir(subfolder_path):\n",
    "                image_folder = subfolder_path / 'image'\n",
    "                label_folder = subfolder_path / 'label_image'\n",
    "\n",
    "                # Get the set of filenames in the 'image' folder\n",
    "                image_files = sorted(set(os.listdir(image_folder)))\n",
    "\n",
    "                # Get the set of filenames in the 'label_image' folder\n",
    "                label_files = sorted(set(os.listdir(label_folder)))\n",
    "\n",
    "                # Find the filenames that are in 'label_image' but not in 'image'\n",
    "                missing_files = [i for i in image_files if i not in label_files]\n",
    "\n",
    "                # Create a black image for each missing file\n",
    "                for file in missing_files:\n",
    "                    file_path = label_folder / file\n",
    "\n",
    "                    # find the shape of the input image to create corresponding target\n",
    "                    file_shape = cv2.imread(str(image_folder / file)).shape\n",
    "                    black_image = np.zeros(file_shape)\n",
    "\n",
    "                    # make unique names so files can be deleted again\n",
    "                    cv2.imwrite(f\"{str(file_path)[:-4]}_empty.png\", black_image)\n",
    "\n",
    "    # Delete the generated black images if delete_images flag is True\n",
    "    if delete_images:\n",
    "        for subfolder in sorted(os.listdir(main_folder)):\n",
    "            subfolder_path = main_folder / subfolder\n",
    "            if os.path.isdir(subfolder_path) and 'label_image' in os.listdir(subfolder_path):\n",
    "                label_folder = subfolder_path / 'label_image'\n",
    "                for file in os.listdir(label_folder):\n",
    "                    file_path = label_folder / file\n",
    "                    if \"_empty\" in str(file):\n",
    "                        os.remove(str(file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_black_images(train_dir, delete_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mOCTDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_path: \u001b[38;5;28mstr\u001b[39m, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, generate_empty_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, source_domain\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpectralis\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path \u001b[38;5;241m=\u001b[39m Path(data_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class OCTDataset(Dataset):\n",
    "    def __init__(self, data_path: str, transform=None, generate_empty_labels=False, source_domain='Spectralis'):\n",
    "\n",
    "        self.data_path = Path(data_path)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.source_domain = source_domain\n",
    "\n",
    "        if generate_empty_labels:\n",
    "            self.generate_black_images(data_path) # check if this misses arguments\n",
    "                                                  # change method so it only creates black images for source domain\n",
    "\n",
    "        self.source_data_paths, self.target_data_paths = self.filter_source_domain(data_path, source_domain=source_domain)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_data_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_img = load_source_image(self.source_data_paths[index])\n",
    "        target_img = load_target_image(self.target_data_paths[index])\n",
    "        \n",
    "        if self.transform:\n",
    "            # Apply transformations dynamically during training\n",
    "            source_img, target_img = self.transform(source_img, target_img)\n",
    "\n",
    "        return source_img, target_img\n",
    "\n",
    "    def filter_source_domain(self, source_domain):\n",
    "        # go into path:\n",
    "        # '/Users/moritz/Documents/Master/OPTIMA_Masterarbeit/data/RETOUCH/TrainingSet-Release/'\n",
    "        # and set the containing folders as keys of a dictionary. The values are the folder names in the respective\n",
    "        # folder.\n",
    "        # The result should be a data structure holding each domain name and the image folder names that stem from that domain.\n",
    "        # e.g. {'cirrus':['path1', 'path2', ...], 'topcon':['path1', 'path2', ...]}\n",
    "        # the method returns the image folder names for the source domain and the target domains separately\n",
    "\n",
    "        named_domain_folder = Path('/Users/moritz/Documents/Master/OPTIMA_Masterarbeit/data/RETOUCH/TrainingSet-Release')\n",
    "        target_domains = os.listdir(named_domain_folder)\n",
    "        target_domains.remove(source_domain)\n",
    "\n",
    "        # creates dict e.g. {'cirrus':['path1', 'path2', ...], 'topcon':['path1', 'path2', ...]}\n",
    "        filter_dict = {domain:os.listdir(named_domain_folder / domain) for domain in os.listdir(named_domain_folder)}\n",
    "\n",
    "        source_data_paths = {source_domain: [img for img in filter_dict[source_domain]]}\n",
    "        target_data_paths = {target_domain: [img for img in filter_dict[target_domain]] for target_domain in target_domains}\n",
    "\n",
    "        return source_data_paths, target_data_paths\n",
    "\n",
    "\n",
    "\n",
    "    def load_source_image(self, source_path):\n",
    "        # Load the source image\n",
    "        return source_img\n",
    "\n",
    "    def load_target_image(self, target_path):\n",
    "        # Load the target image\n",
    "        return target_img\n",
    "\n",
    "\n",
    "    def generate_black_images(self, main_folder, delete_images=False):\n",
    "        \"\"\"\n",
    "        Generate black images for missing files in the label_image folder.\n",
    "\n",
    "        Args:\n",
    "            main_folder (str or Path): Path to the main folder.\n",
    "            delete_images (bool, optional): Flag to delete the generated black images. Defaults to False.\n",
    "        \"\"\"\n",
    "\n",
    "        if not delete_images:\n",
    "        # Iterate over the subfolders\n",
    "            for subfolder in sorted(os.listdir(main_folder)):\n",
    "                subfolder_path = main_folder / subfolder\n",
    "\n",
    "                # Check if the subfolder contains the 'image' and 'label_image' folders\n",
    "                if os.path.isdir(subfolder_path) and 'image' in os.listdir(subfolder_path) and 'label_image' in os.listdir(subfolder_path):\n",
    "                    image_folder = subfolder_path / 'image'\n",
    "                    label_folder = subfolder_path / 'label_image'\n",
    "\n",
    "                    # Get the set of filenames in the 'image' folder\n",
    "                    image_files = sorted(set(os.listdir(image_folder)))\n",
    "\n",
    "                    # Get the set of filenames in the 'label_image' folder\n",
    "                    label_files = sorted(set(os.listdir(label_folder)))\n",
    "\n",
    "                    # Find the filenames that are in 'label_image' but not in 'image'\n",
    "                    missing_files = [i for i in image_files if i not in label_files]\n",
    "\n",
    "                    # Create a black image for each missing file\n",
    "                    for file in missing_files:\n",
    "                        file_path = label_folder / file\n",
    "\n",
    "                        # find the shape of the input image to create corresponding target\n",
    "                        file_shape = cv2.imread(str(image_folder / file)).shape\n",
    "                        black_image = np.zeros(file_shape)\n",
    "\n",
    "                        # make unique names so files can be deleted again\n",
    "                        cv2.imwrite(f\"{str(file_path)[:-4]}_empty.png\", black_image)\n",
    "\n",
    "        # Delete the generated black images if delete_images flag is True\n",
    "        if delete_images:\n",
    "            for subfolder in sorted(os.listdir(main_folder)):\n",
    "                subfolder_path = main_folder / subfolder\n",
    "                if os.path.isdir(subfolder_path) and 'label_image' in os.listdir(subfolder_path):\n",
    "                    label_folder = subfolder_path / 'label_image'\n",
    "                    for file in os.listdir(label_folder):\n",
    "                        file_path = label_folder / file\n",
    "                        if \"_empty\" in str(file):\n",
    "                            os.remove(str(file_path))\n",
    "\n",
    "    def delete_generated_labels(self):\n",
    "        # Delete the generated black images\n",
    "        self.generate_black_images(delete_images=True)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Define the randomizable transformation class for dynamic transformations\n",
    "class RandomTransform(RandomizableTransform):\n",
    "    def __call__(self, source_img, target_img):\n",
    "        self.randomize()\n",
    "        # Implement your dynamic random transformations\n",
    "        # Apply random transformations to both source and target images\n",
    "        return transformed_source_img, transformed_target_img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the UNet model\n",
    "class SegmentationUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegmentationUNet, self).__init__()\n",
    "        # Define your UNet architecture using MONAI\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement your forward pass logic\n",
    "        return x\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ready = False\n",
    "\n",
    "if training_ready:\n",
    "\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"PracticalWorkinAI\", name=\"svdna_reproduction_retouch_only\")\n",
    "\n",
    "    # Instantiate the model, loss function, and optimizer\n",
    "    model = SegmentationUNet()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Define the dataset paths\n",
    "    source_data_paths = [...]  # List of paths to source domain data\n",
    "    target_data_paths = [...]  # List of paths to target domain data\n",
    "\n",
    "    # Create the dataset and dataloader\n",
    "    transform = Compose([\n",
    "        RandomTransform(),\n",
    "        svdna,  # Apply SVDNA function\n",
    "        ToTensord()\n",
    "    ])\n",
    "\n",
    "    dataset = OCTDataset(source_data_paths, target_data_paths, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (source_img, target_img) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(source_img)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output, target_img)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log loss to wandb\n",
    "            wandb.log({\"Loss\": loss.item(), \"Epoch\": epoch, \"Batch\": batch_idx})\n",
    "\n",
    "            # Print progress\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item()}')\n",
    "\n",
    "    # Save the trained model if needed\n",
    "    torch.save(model.state_dict(), 'segmentation_unet.pth')\n",
    "\n",
    "    # Finish wandb run\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_domain = 'Spectralis'\n",
    "data_path = '/Users/moritz/Documents/Master/OPTIMA_Masterarbeit/data/Retouch-Preprocessed/train/'\n",
    "named_domain_folder = Path('/Users/moritz/Documents/Master/OPTIMA_Masterarbeit/data/RETOUCH/TrainingSet-Release/')\n",
    "filter_dict = {domain:os.listdir(named_domain_folder / domain) for domain in os.listdir(named_domain_folder)}\n",
    "source_data_paths = {source_domain: [img for img in filter_dict[source_domain] if img in os.listdir(data_path)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Spectralis': ['f520e52af71723efed0d6fecf934075b',\n",
       "  '4223bd8c7002828a266d03b666b5c492',\n",
       "  'dccf9863a1914de55b78a98f0791f605',\n",
       "  'fe02f982b78218ab05c755c01c7876b1',\n",
       "  'be0a143d6181d7f4f366fe2c0cc23075',\n",
       "  '7501081e3e7577af524c6f7703d8d538',\n",
       "  '11f08e45fcac03ea64b7138754117435',\n",
       "  'c7e339f972f58b5388fbeb13adc670eb',\n",
       "  '83e7b9339019b549c87830f90c560fea',\n",
       "  '7096ef65c25c1c87068ed9cdf2b73a59',\n",
       "  'f4b90ca25b223e4598c32caab90dc4aa',\n",
       "  'bd14b46237f99db413f29d4797f246e1',\n",
       "  '8844486c9f1b8952d0988d0acbea86c8',\n",
       "  '92a92b80b8a4c48dc4775dd60819cf3b',\n",
       "  'af3868172d16615c9556cacabcc80d66',\n",
       "  '7b2607e057592d507c4ec4732bae64c2',\n",
       "  '4a8a81b1c06072385738775dccdc7942',\n",
       "  '76ebaa858a59427f392d97ed4b894f6d',\n",
       "  '8945696b67c38140f30e99c0050aa925',\n",
       "  'ecf4cb44944f518c82a4f108b94b0dfd',\n",
       "  'd1ad979857c1877496207f3d4e5f5c52',\n",
       "  '88bab0b300b19bdac3dc431cb3507a2a',\n",
       "  '7456e703d778e17f29e42f97615f4a68',\n",
       "  '7a05c267fdde4c819b19eb30da70d387']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_data_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([3, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = [{'img':[1,2,3], 'label':[1,2,3]}, {'img':3, 'label':4}, {'img':5, 'label':6}, {'img':7, 'label':8}]\n",
    "d[1].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/moritz/Documents/Master/OPTIMA_Masterarbeit/practical/data/RETOUCH/TrainingSet-Release')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When applying SVDNA, be careful that not the same target images are chosen every epoch.\n",
    "Path(Path.cwd() / 'data/RETOUCH/TrainingSet-Release')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 24]\n"
     ]
    }
   ],
   "source": [
    "def filter_source_domain(source_domain):\n",
    "    # go into path:\n",
    "    # '/Users/moritz/Documents/Master/OPTIMA_Masterarbeit/data/RETOUCH/TrainingSet-Release/'\n",
    "    # and set the containing folders as keys of a dictionary. The values are the folder names in the respective\n",
    "    # folder.\n",
    "    # The created dictionary serves as a tool for filtering the folders that contain the source images.\n",
    "    # The folder \"self.data_path\" contains folder names that correspond to the folder names in the dictionary's values.\n",
    "    # The goal is to return a variable source_data_paths containing the paths to the source domain images, while \n",
    "    # the variable target_data_paths contains the paths to the target domain images (Cirrus and Topcon).\n",
    "    # source_data_paths should be a dictionary with the structure {Spectralis: [path1, path2, ...]}\n",
    "    # target_data_paths should be a dictionary with the structure {Cirrus: [path1, path2, ...], Topcon: [path1, path2, ...]}\n",
    "    # The dictionary should be created in the __init__ method of the class.\n",
    "    # The method should return the source_data_paths and target_data_paths variables.\n",
    "\n",
    "    named_domain_folder = Path('/Users/moritz/Documents/Master/OPTIMA_Masterarbeit/data/RETOUCH/TrainingSet-Release')\n",
    "    target_domains = os.listdir(named_domain_folder)\n",
    "    target_domains.remove(source_domain)\n",
    "\n",
    "    # creates dict e.g. {'cirrus':['path1', 'path2', ...], 'topcon':['path1', 'path2', ...]}\n",
    "    filter_dict = {domain:os.listdir(named_domain_folder / domain) for domain in os.listdir(named_domain_folder)}\n",
    "\n",
    "    source_data_paths = {source_domain: [img for img in filter_dict[source_domain]]}\n",
    "    target_data_paths = {target_domain: [img for img in filter_dict[target_domain]] for target_domain in target_domains}\n",
    "\n",
    "    return source_data_paths, target_data_paths\n",
    "    \n",
    "\n",
    "filter_source_domain('Spectralis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVDNA_demo_updated.ipynb   main.ipynb\n",
      "\u001b[34mdata\u001b[m\u001b[m                       main.py\n",
      "layer_transformation(1).py unet_training_dict.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optima",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
